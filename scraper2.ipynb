{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] - Starting TitanScraper v0.0.21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from titanscraper import TitanScraper\n",
    "from titanscraper.processors import *\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "scraper = TitanScraper()\n",
    "\n",
    "webonary = \"https://www.webonary.org/pular/browse/browse-pular-francais/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet links scraped successfully \n"
     ]
    }
   ],
   "source": [
    "# selectionner les onglets (alphabet)\n",
    "alphabet_links = scraper.get_links_from_page(webonary, '', '.lpTitleLetter')\n",
    "with open('collected/alphabet.json', 'w') as f:\n",
    "    json.dump(alphabet_links, f)\n",
    "    \n",
    "print('Alphabet links scraped successfully ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apres avoir reccuperé les lettres de l'alphabet il faut à present pour chaque lettre reccuperer tous les onglets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_file(new_data, file_path):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        f = open(file_path, 'w')\n",
    "        f.write('[]')\n",
    "        f.close()\n",
    "    with open(file_path, 'r+') as file:\n",
    "        file_data = json.load(file)\n",
    "        file_data += new_data\n",
    "        file.seek(0)\n",
    "        json.dump(file_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_links = []  # letter index and page index\n",
    "for letter_link in alphabet_links[last[0]:]:\n",
    "\n",
    "    letter_index, page_index = last[0]+1, last[1]\n",
    "\n",
    "    # create the folder where we will store dictionary words links for each page of the letter\n",
    "    page_path = os.path.join(\n",
    "        os.getcwd(), f'collected/letters/letter_{letter_index}')\n",
    "    if not os.path.exists(page_path):\n",
    "        os.mkdir(page_path)\n",
    "\n",
    "    while True:\n",
    "        # Getting all dictionnary words links in the current page\n",
    "        page_target = f\"{letter_link}&pagenr={page_index}\"\n",
    "        page_links: list[str] = scraper.get_links_from_page(\n",
    "            page_target, '', '.entry a')\n",
    "        if len(page_links) == 0:\n",
    "            last[1] = 1\n",
    "            break\n",
    "        page_links = [link.removeprefix(page_target) for link in page_links]\n",
    "        with open(os.path.join(page_path, f'page_{page_index}.json'), 'w') as page_links_file:\n",
    "            json.dump(page_links, page_links_file)\n",
    "\n",
    "        # update files links\n",
    "        update_json_file(page_links, os.path.join(\n",
    "            page_path, f'all_letter_links.json'))\n",
    "        update_json_file(page_links, 'collected/all_words_links.json')\n",
    "\n",
    "        print(\n",
    "            f\"[{letter_index}/{len(alphabet_links)}][{page_index}] New page page parsed ...\")\n",
    "        page_index += 1\n",
    "        last[1]+=1\n",
    "    last[0] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RULES = [\n",
    "\n",
    "    {\n",
    "        \"name\": \"pl\",\n",
    "        \"selector\": \".mainheadword\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fr\",\n",
    "        \"selector\": \".definitionorgloss\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gr\",\n",
    "        \"selector\": \".sharedgrammaticalinfo\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"pl_ex\",\n",
    "        \"selector\": \".example\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fr_ex\",\n",
    "        \"selector\": \".translation\",\n",
    "    }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les liens vers les differents mots reccupérés, il faut à present reconstituer le dictionnaire en reccuperant les données de chaque mot, et les exemples associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('collected/all_words_links.json', 'r') as file:\n",
    "    targets = json.load(file)\n",
    "targets.reverse()\n",
    "sl = slice(0, 25)\n",
    "size = 25\n",
    "has_finished = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/24556] scraped ....\n",
      "[50/24556] scraped ....\n",
      "[75/24556] scraped ....\n",
      "[100/24556] scraped ....\n",
      "[125/24556] scraped ....\n",
      "[150/24556] scraped ....\n",
      "[175/24556] scraped ....\n"
     ]
    }
   ],
   "source": [
    "while not has_finished:\n",
    "    if sl.stop >= len(targets)//2:\n",
    "        has_finished = True\n",
    "    try:\n",
    "        data: list[dict] = scraper.scrap(targets[sl], RULES)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        update_json_file(data, 'dico_reversed.json')\n",
    "\n",
    "        borns = [sl.start+size, min([sl.stop + size, len(targets)+1])]\n",
    "        print(f\"[{sl.stop}/{len(targets)}] scraped ....\")\n",
    "        sl = slice(borns[0], borns[1])\n",
    "        with open('borns_reversed.txt', 'w') as f:\n",
    "            f.write(f\"{borns[0]},{borns[1]}\")\n",
    "\n",
    "print(\"Reverse Job done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9ebd3ff607729a7cbb3fb982ffff5cbd5fec53b6fd8f